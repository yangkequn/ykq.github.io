---
title: "文本自动摘要算法"
date: 2021-04-01T19:16:36+08:00
draft: false
---

### 传统算法
* lead-3. 打头的三句话。看起来百度在用
* TexRank。 是和谷歌pagerank 很接近的实现算法
> [TextRank4ZH](https://github.com/letiantian/TextRank4ZH "工具1")
* 谷歌算法，摘取最熵值最大的连续片段。
* LexRank，一种类似于TextRank的无监督方法。
LexRank使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能的后处理步骤，确保为摘要选择的顶级句子彼此不太相似。
* KL-Sum [一种启发性的最长公共子序列贪心匹配摘要算法 ](https://kexue.fm/archives/8209 "一种贪心匹配摘要算法")
* LSA，包括潜在语义分析，测试效果良好
### 实用的自动摘要工具包
* [sumy ](https://github.com/miso-belica/sumy "sumy ")  是github上最受欢迎的工具包。以上这些方法开箱可用，细节见 [ LSA，LexRank和TexRank,SumBasic，KL-Sum ](https://github.com/miso-belica/sumy/blob/master/docs/summarizators.md "使用的方法")
* [nlg-yongzhuo ](https://github.com/yongzhuo/nlg-yongzhuo "nlg-yongzhuo ") ，中文文本生成，同样包含许多传统算法，未能成功安装
***

### 基于神经网络和深度学习的摘要方案
* [文本摘要简述](https://zhuanlan.zhihu.com/p/84734784 "文本摘要简述")
生成式摘要面临的一些常见问题：但是简单的Seq2seq直接应用到摘要生成会有一些问题，比如重复生成、信息冗余，无法处理未登录词，关键信息丢失，可读性差等等。相对应的改进主要有以下几类：
* 从16年~19年，摘要任务持续走热，提出的新模型也是不胜枚举。左右摘要任务性能的关键点到底在哪呢？ACL19这篇文章对此做了探讨。 [Searching for Effective Neural Extractive Summarization: What Works and What’s Next](http://pfliu.com/InterpretSum/interpretSum.html "Searching for Effective Neural Extractive Summarization: What Works and What’s Next") 

***
### 一些基于深度学习的自动摘要的SOTA
#### Textsum
2016 年，谷歌也开源了基于 TensorFlow的一个自动摘要模块 Textsum

#### UniLM ，微软，2020
* [微软AI模型UniLM 在摘要和语言生成上实现超越](https://www.shuzix.com/17176.html "微软AI模型UniLM 在摘要和语言生成上实现超越")

#### BertSum, 2019 年的SOTA
* [github code](https://github.com/nlpyang/BertSum "代码实现") ,  [arxiv paper](https://arxiv.org/pdf/1903.10318.pdf "论文"),  https://zhuanlan.zhihu.com/p/112282988

#### matchsum，复旦大学，2020年的SOTA
* [github code](https://github.com/maszhongming/MatchSum "code")， [arxiv paper](https://arxiv.org/pdf/2004.08795.pdf "arxiv")，  [作者自己的视频讲解](https://www.bilibili.com/video/av200402243/ "知乎上作者自己的讲解视频")

#### PEGASUS，2020年google research 的SOTA，给出了超越人类水平的摘要性能！！！
*  [论文地址](https://arxiv.org/abs/1912.08777 "论文地址"), [论文的中文翻译](http://www.weainfo.net/news/detail/461443 "论文的中文翻译"),  [官方博客地址](https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html "博客地址")

* [实现代码](https://github.com/google-research/pegasus "实现代码"),  [非官方tensorflow 2.2版本 ](https://github.com/TheRockXu/pegasus-demo "非官方tensorflow 2.2版本 ")，[非官方tensorflow 2.3版本]( https://github.com/bondarchukb/PEGASUS "非官方tensorflow 2.3版本")， [中文版本的pegasus,一次性生成缺失的1/4文本](https://github.com/ZhuiyiTechnology/t5-pegasus "中文版本的pegasus,")
##### 总言之，PEGASUS 谷歌工具中穷人版的好东西。对它有挺高期望，也花了不少时间研究。但由于它苛刻的生成形式，并不觉得可以作为摘要生成来使用
2021 - 04  -01 文本摘要
---
2021-08-21 22:27:32 星期六
超级高效，好用的文本摘要算法
https://blog.csdn.net/malefactor/article/details/51264244
