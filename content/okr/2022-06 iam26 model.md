---
title: "study Reinforcement Learning"
date: 2022-06-24T11:02:45+08:00
draft: false
---

# 6月主要工作：在学习强化学习。
    书学了两本。算是入门，研究各种最新SOTA技术。强化学习框架
    最后打算用actor-critic框架来做。
actor critic 模型
``` mermaid
flowchart LR
    enviroment --> observation
    observation --> Hearbeat --HearbeatVAE -->HBZ
    observation --> Audio
    HBZ --> MDN-RNN(Dynamic,NextState)  --> critic    
    Audio --AudioVAE --> AZ --> MDN-RNN
    HBZ-->critic
    AZ-->critic
    enviroment --reward--> critic
```
 
### 世界模型 https://zhuanlan.zhihu.com/p/42537455
### https://blog.csdn.net/KuXiaoQuShiHuai/article/details/109657951 看vae
### https://worldmodels.github.io/
### https://zhuanlan.zhihu.com/p/384420701 DreamerV2实现代码
### https://zhuanlan.zhihu.com/p/363774920 DreamerV2 讲得很详细
### https://zhuanlan.zhihu.com/p/34998569 VAE的实现原理 https://github.com/bojone/vae/blob/master/cvae_keras.py
其中提到Action 作用于隐态，得下一个状态，应该是一个独立函数，隐态之间遵守 min KL
``` mermaid
flowchart LR
    enviroment --> observation
    observation --> Hearbeat --HearbeatVAE -->HBZ --> MDN-RNN(Dynamic,NextState)  --> controller    

    controller --> ActionZ --> MDN-RNN
    HBZ-->controller
    ActionZ --> enviroment 
```
``` mermaid
flowchart LR
    enviroment --> observation
    observation --> Hearbeat --HearbeatVAE -->HBZ --> MDN-RNN(Dynamic,NextState)  --> controller    

    controller --> ActionZ --> MDN-RNN
    controller --> Reward
    controller --> Value
    HBZ-->controller
    ActionZ --> enviroment 
```
回报如何引入。如果改进模型
Dreamer v2 是否是同一个东西，如何改进

用于声音的两点改进
1.使用sgd
2.正则化约束使用很低的权重




``` mermaid 已经放弃
flowchart LR
    enviroment --> observation
    observation --> Hearbeat --HearbeatVAE -->HBZ --> MDN-RNN(Dynamic,NextState) 
    observation --> Audio --AudioVAE -->AZ --> MDN-RNN(Dynamic,NextState)
    AZ --> controller    
    MDN-RNN(Dynamic,NextState)  --> controller    

   
    HBZ-->controller
    controller-->NewAZ --> enviroment 
```

