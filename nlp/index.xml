<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Nlps on To Build, I Live</title><link>https://kequnyang.com/nlp/</link><description>Recent content in Nlps on To Build, I Live</description><generator>Hugo -- gohugo.io</generator><language>zh</language><lastBuildDate>Wed, 01 Sep 2021 21:58:16 +0800</lastBuildDate><atom:link href="https://kequnyang.com/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>text2vec 与矢量索引引擎</title><link>https://kequnyang.com/nlp/text2vec-%E4%B8%8E%E7%9F%A2%E9%87%8F%E7%B4%A2%E5%BC%95%E5%BC%95%E6%93%8E/</link><pubDate>Wed, 01 Sep 2021 21:58:16 +0800</pubDate><guid>https://kequnyang.com/nlp/text2vec-%E4%B8%8E%E7%9F%A2%E9%87%8F%E7%B4%A2%E5%BC%95%E5%BC%95%E6%93%8E/</guid><description>1.milvus Milvus 是一款开源的、针对海量特征向量的相似性搜索引擎。 https://github.com/milvus-io/milvus 文档 https://milvus.io/cn/
各种各样的语料库，非常全，而且可以下载 冷眼-风雨飘摇 专注于python、自然语言处理 https://cold-eye.github.io/post/nlp-corpus/ 2021-09-01 13:39:39 星期三
text2vec引擎 推理速度可以满足实时性要求。https://github.com/NVIDIA/FasterTransformer ELECTRA模型 https://www.leiphone.com/category/academic/i2yH9anJWkh8rd6r.html 中文预训练模型 https://www.leiphone.com/category/academic/i2yH9anJWkh8rd6r.html https://github.com/ymcui/Chinese-ELECTRA 科大讯飞某个主要技术负责人的github https://github.com/ymcui 中文训练数据集扩大9倍后的效果 https://www.jiqizhixin.com/articles/2020-10-26-11
苏剑林的反省：吐槽贴：用ELECTRA、ALBERT之前，你真的了解它们吗？ https://bbs.huaweicloud.com/blogs/226675 苏剑林的博客 https://kexue.fm/
对bert 进行蒸馏，tinyBERT,推理速度提高9倍 https://zhuanlan.zhihu.com/p/94359189 这是很棒的模型，一来可以直接训练，二来，可以用来蒸馏后使用
最后，github 上找出了一个质量很不错的答案，来解决计算文本相似度的问题 https://github.com/shibing624/text2vec 具体实现包括wordVector 平均值法，以及small Bert 模型，重要的是这个是开箱可用的 https://www.sbert.net/index.html
2021-09-02 15:06:43 星期四 # fasterTransformer https://baike.baidu.com/item/Faster%20Transformer/23737285?fr=aladdin
最新的媲美Bert的模型 PRADO，pQRNN https://zhuanlan.zhihu.com/p/257934777 ：印象是虽然小，但是没有放出的代码实现。也没有中文版本的踩坑记录
2021-09-01 #</description></item><item><title>文本相似度分析</title><link>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%88%86%E6%9E%90/</link><pubDate>Tue, 22 Jun 2021 19:16:16 +0800</pubDate><guid>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%88%86%E6%9E%90/</guid><description> Method1：平均word-vector法 # 对求句子中所有单词词嵌入的平均值，然后计算两句子词嵌入之间的余弦相似性 参考 中文文本相似度计算
个人觉得，可以用Zipf 假定，对不同的单词向量做调权处理。这样也不用管什么stop words 之类的问题。权重嘛，-*log(p) Methods2: 传统的TF-IDF计算文本相似度 # 分析工具 gensim
优点包括： 1. 速度快 2.可以保存训练好的模型 3.用户多，代码成熟 4.可以训练词向量模型 Method3: Doc2vec法 # Doc2Vec实际上计算除了一个总体语义。总体语义都有了，这意味着很多。</description></item><item><title>文本关键词提取</title><link>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/</link><pubDate>Sat, 17 Apr 2021 17:00:16 +0800</pubDate><guid>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/</guid><description>文本关键词生产算法的目标。 # 在某种先验之下，无论对一篇文章，还是一句话，都能准确提取出其中的关键词 doc2vec 算法 # dock2vec 计算出了总体语义，那这个总体语义就是关键词的等效。 gensim 提供了dock2vec 的实现。很棒。示例 一个预先训练好的doc2vec 实现 gensim TaggedDocumen # 在doc2vec的基础上，扩展一个标签向量，对许多标签都行训练，然后选择与文档向量最接近的标签向量作为标签 这个办法的标签选择准确性高达74%，是很棒的方法 实现1 代码实现2 基于fasttext 的关键词提取 # 速度是textrank4zh的8倍。github 代码 基于TF-IDF算法进行关键词提取 # TF-IDF是一个很强的基准。没有特别的情况下，可以选择作为实现方式。 结巴分词、scikit-learn、gensium 都有对应的实现 jieba分词：jieba.analyse.extract_tags(sentence, topK=20, withWeight=False,allowPOS=()) 3种库的比较： https://www.solarck.com/compare-tfidf.html
基于TextRank算法 # 结巴分词：jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&amp;rsquo;ns&amp;rsquo;, &amp;rsquo;n&amp;rsquo;, &amp;lsquo;vn&amp;rsquo;, &amp;lsquo;v&amp;rsquo;)) textrank4zh TextRank实际应用效果并不比TFIDF有明显优势，而且由于涉及网络构建和随机游走的迭代算法，效率极低 基于Word2Vec词聚类的文本关键词抽取方法 # 参考 效果：差 主要思路是对于用词向量表示的文本词语，通过K-Means算法对文章中的词进行聚类，选择聚类中心作为文章的一个主要关键词，计算其他词与聚类中心的距离即相似度，选择topN个距离聚类中心最近的词作为文本关键词，而这个词间相似度可用Word2Vec生成的向量计算得到。scikit-learn 可提供聚类计算框架 Topic Model # 模型较复杂。它的主要问题是抽取的关键词一般过于宽泛，不能较好反映文章主题。 有监督学习 # 通常效果会比较好</description></item><item><title>文本自动摘要算法</title><link>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E7%AE%97%E6%B3%95/</link><pubDate>Thu, 01 Apr 2021 19:16:36 +0800</pubDate><guid>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E7%AE%97%E6%B3%95/</guid><description>传统算法 # lead-3. 打头的三句话。看起来百度在用 TexRank。 是和谷歌pagerank 很接近的实现算法 TextRank4ZH
谷歌算法，摘取最熵值最大的连续片段。 LexRank，一种类似于TextRank的无监督方法。 LexRank使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能的后处理步骤，确保为摘要选择的顶级句子彼此不太相似。 KL-Sum 一种启发性的最长公共子序列贪心匹配摘要算法 LSA，包括潜在语义分析，测试效果良好 实用的自动摘要工具包 # sumy 是github上最受欢迎的工具包。以上这些方法开箱可用，细节见 LSA，LexRank和TexRank,SumBasic，KL-Sum nlg-yongzhuo ，中文文本生成，同样包含许多传统算法，未能成功安装 基于神经网络和深度学习的摘要方案 # 文本摘要简述 生成式摘要面临的一些常见问题：但是简单的Seq2seq直接应用到摘要生成会有一些问题，比如重复生成、信息冗余，无法处理未登录词，关键信息丢失，可读性差等等。相对应的改进主要有以下几类： 从16年~19年，摘要任务持续走热，提出的新模型也是不胜枚举。左右摘要任务性能的关键点到底在哪呢？ACL19这篇文章对此做了探讨。 Searching for Effective Neural Extractive Summarization: What Works and What’s Next 一些基于深度学习的自动摘要的SOTA # Textsum # 2016 年，谷歌也开源了基于 TensorFlow的一个自动摘要模块 Textsum
UniLM ，微软，2020 # 微软AI模型UniLM 在摘要和语言生成上实现超越 BertSum, 2019 年的SOTA # github code , arxiv paper, https://zhuanlan.zhihu.com/p/112282988 matchsum，复旦大学，2020年的SOTA # github code， arxiv paper， 作者自己的视频讲解 PEGASUS，2020年google research 的SOTA，给出了超越人类水平的摘要性能！！！ # 论文地址, 论文的中文翻译, 官方博客地址</description></item><item><title>文本采样调研 - 文本生成</title><link>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E9%87%87%E6%A0%B7%E8%B0%83%E7%A0%94-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/</link><pubDate>Wed, 17 Mar 2021 09:19:16 +0800</pubDate><guid>https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E9%87%87%E6%A0%B7%E8%B0%83%E7%A0%94-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/</guid><description>这个工作尝试使用文本生成的内容作为学习内容。看看这个思路是否具有可行性 # 文本生成的相关工具 # MobileBERT tensorflow 文本处理相关 https://www.tensorflow.org/tutorials/text/transformer
Gpt-2 # Gpt-2 在线测试。 https://deepai.org/machine-learning-model/text-generator https://transformer.huggingface.co/doc/distil-gpt2
Gpt-2 的测试结构表明： 没有fine-tune, 文本生成的内容绝对不能作为学习材料，因为可靠性太差。
Gpt-3 # Gpt-3的测试 案例：用于媒体广告灵感生成 测试地址
像这样的生产，不是以文字打头，而是用基于内容的随机采样，是可以用于生成初始的讲解数据的 但是还是有大量不相干的内容采样。需要平台整理，
我意识到，文本生成的内容既不可控又不可靠。用于产生学习的内容, 其实需要的并不是文本采样工具。文本生成只是我接触到的相关的概念。但是我需要什么样的工具应该重新分析
VAE 和 SEQ2SEQ 的文本生成 # 【NLP笔记】文本生成基础与方案梳理 评： * 指定关键词生成内容是可能的。 * 但是是不是总是生成 * 逻辑上是否合理，没有保证 * 但是只要有一些显然是合理的生成内容，就可以部分解决问题 文本生成概述 2021-10-05 23:52:50 星期二
文本生成的实现办法，这个视频讲了挺多实现办法 # https://uai.greedyai.com/ai-open-courses/the-generation-problem-in-the-dialogue-system
10/06/2021 hugging face 提供的，基于GPT-2的文本生成 # https://huggingface.co/gpt2?text=A+long+time+ago%2C
2021-10-30 13:16:31</description></item></channel></rss>