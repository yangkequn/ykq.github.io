<!doctype html><html lang=en dir=" ltr "><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="传统算法 # lead-3. 打头的三句话。看起来百度在用 TexRank。 是和谷歌pagerank 很接近的实现算法 TextRank4ZH
谷歌算法，摘取最熵值最大的连续片段。 LexRank，一种类似于TextRank的无监督方法。 LexRank使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能的后处理步骤，确保为摘要选择的顶级句子彼此不太相似。 KL-Sum 一种启发性的最长公共子序列贪心匹配摘要算法 LSA，包括潜在语义分析，测试效果良好 实用的自动摘要工具包 # sumy 是github上最受欢迎的工具包。以上这些方法开箱可用，细节见 LSA，LexRank和TexRank,SumBasic，KL-Sum nlg-yongzhuo ，中文文本生成，同样包含许多传统算法，未能成功安装 基于神经网络和深度学习的摘要方案 # 文本摘要简述 生成式摘要面临的一些常见问题：但是简单的Seq2seq直接应用到摘要生成会有一些问题，比如重复生成、信息冗余，无法处理未登录词，关键信息丢失，可读性差等等。相对应的改进主要有以下几类： 从16年~19年，摘要任务持续走热，提出的新模型也是不胜枚举。左右摘要任务性能的关键点到底在哪呢？ACL19这篇文章对此做了探讨。 Searching for Effective Neural Extractive Summarization: What Works and What’s Next 一些基于深度学习的自动摘要的SOTA # Textsum # 2016 年，谷歌也开源了基于 TensorFlow的一个自动摘要模块 Textsum
UniLM ，微软，2020 # 微软AI模型UniLM 在摘要和语言生成上实现超越 BertSum, 2019 年的SOTA # github code , arxiv paper, https://zhuanlan.zhihu.com/p/112282988 matchsum，复旦大学，2020年的SOTA # github code， arxiv paper， 作者自己的视频讲解 PEGASUS，2020年google research 的SOTA，给出了超越人类水平的摘要性能！！！ # 论文地址, 论文的中文翻译, 官方博客地址"><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="文本自动摘要算法"><meta property="og:description" content="传统算法 # lead-3. 打头的三句话。看起来百度在用 TexRank。 是和谷歌pagerank 很接近的实现算法 TextRank4ZH
谷歌算法，摘取最熵值最大的连续片段。 LexRank，一种类似于TextRank的无监督方法。 LexRank使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能的后处理步骤，确保为摘要选择的顶级句子彼此不太相似。 KL-Sum 一种启发性的最长公共子序列贪心匹配摘要算法 LSA，包括潜在语义分析，测试效果良好 实用的自动摘要工具包 # sumy 是github上最受欢迎的工具包。以上这些方法开箱可用，细节见 LSA，LexRank和TexRank,SumBasic，KL-Sum nlg-yongzhuo ，中文文本生成，同样包含许多传统算法，未能成功安装 基于神经网络和深度学习的摘要方案 # 文本摘要简述 生成式摘要面临的一些常见问题：但是简单的Seq2seq直接应用到摘要生成会有一些问题，比如重复生成、信息冗余，无法处理未登录词，关键信息丢失，可读性差等等。相对应的改进主要有以下几类： 从16年~19年，摘要任务持续走热，提出的新模型也是不胜枚举。左右摘要任务性能的关键点到底在哪呢？ACL19这篇文章对此做了探讨。 Searching for Effective Neural Extractive Summarization: What Works and What’s Next 一些基于深度学习的自动摘要的SOTA # Textsum # 2016 年，谷歌也开源了基于 TensorFlow的一个自动摘要模块 Textsum
UniLM ，微软，2020 # 微软AI模型UniLM 在摘要和语言生成上实现超越 BertSum, 2019 年的SOTA # github code , arxiv paper, https://zhuanlan.zhihu.com/p/112282988 matchsum，复旦大学，2020年的SOTA # github code， arxiv paper， 作者自己的视频讲解 PEGASUS，2020年google research 的SOTA，给出了超越人类水平的摘要性能！！！ # 论文地址, 论文的中文翻译, 官方博客地址"><meta property="og:type" content="article"><meta property="og:url" content="https://kequnyang.com/nlp/%E6%96%87%E6%9C%AC%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E7%AE%97%E6%B3%95/"><meta property="article:section" content="nlp"><meta property="article:published_time" content="2021-04-01T19:16:36+08:00"><meta property="article:modified_time" content="2021-04-01T19:16:36+08:00"><title>文本自动摘要算法 | To Build, I Live</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.dd05bb7cb8e7cc9da19dd8062472a16791465d468ff2c52bf4f137e6d138cbda.css integrity="sha256-3QW7fLjnzJ2hndgGJHKhZ5FGXUaP8sUr9PE35tE4y9o=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script>
<script defer src=/en.search.min.55fbce67ad9711b91181e6285a002d016d17806fcfd5b5a324c3da113b84adf0.js integrity="sha256-VfvOZ62XEbkRgeYoWgAtAW0XgG/P1bWjJMPaETuErfA=" crossorigin=anonymous></script>
<script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=" ltr "><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>To Build, I Live</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/forcasts/ target=_blank rel=noopener>forcasts / 预言</a></li><li><a href=/ideas/ target=_blank rel=noopener>ideas / 想法</a></li><li><a href=/okr/2022-10-meditation/ target=_blank rel=noopener>Todo Last Month</a></li><li><a href=/okr/2022-11-meditation/ target=_blank rel=noopener>Todo This Month</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>文本自动摘要算法</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#传统算法>传统算法</a></li><li><a href=#实用的自动摘要工具包>实用的自动摘要工具包</a></li><li><a href=#基于神经网络和深度学习的摘要方案>基于神经网络和深度学习的摘要方案</a></li><li><a href=#一些基于深度学习的自动摘要的sota>一些基于深度学习的自动摘要的SOTA</a></li></ul></li><li><a href=#2021---04---01-文本摘要>2021 - 04 -01 文本摘要</a></li></ul></li></ul></nav></aside></header><article class=markdown><h3 id=传统算法>传统算法
<a class=anchor href=#%e4%bc%a0%e7%bb%9f%e7%ae%97%e6%b3%95>#</a></h3><ul><li>lead-3. 打头的三句话。看起来百度在用</li><li>TexRank。 是和谷歌pagerank 很接近的实现算法</li></ul><blockquote><p><a href=https://github.com/letiantian/TextRank4ZH title=工具1>TextRank4ZH</a></p></blockquote><ul><li>谷歌算法，摘取最熵值最大的连续片段。</li><li>LexRank，一种类似于TextRank的无监督方法。
LexRank使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能的后处理步骤，确保为摘要选择的顶级句子彼此不太相似。</li><li>KL-Sum <a href=https://kexue.fm/archives/8209 title=一种贪心匹配摘要算法>一种启发性的最长公共子序列贪心匹配摘要算法</a></li><li>LSA，包括潜在语义分析，测试效果良好</li></ul><h3 id=实用的自动摘要工具包>实用的自动摘要工具包
<a class=anchor href=#%e5%ae%9e%e7%94%a8%e7%9a%84%e8%87%aa%e5%8a%a8%e6%91%98%e8%a6%81%e5%b7%a5%e5%85%b7%e5%8c%85>#</a></h3><ul><li><a href=https://github.com/miso-belica/sumy title="sumy ">sumy</a> 是github上最受欢迎的工具包。以上这些方法开箱可用，细节见 <a href=https://github.com/miso-belica/sumy/blob/master/docs/summarizators.md title=使用的方法>LSA，LexRank和TexRank,SumBasic，KL-Sum</a></li><li><a href=https://github.com/yongzhuo/nlg-yongzhuo title="nlg-yongzhuo ">nlg-yongzhuo</a> ，中文文本生成，同样包含许多传统算法，未能成功安装</li></ul><hr><h3 id=基于神经网络和深度学习的摘要方案>基于神经网络和深度学习的摘要方案
<a class=anchor href=#%e5%9f%ba%e4%ba%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%92%8c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%91%98%e8%a6%81%e6%96%b9%e6%a1%88>#</a></h3><ul><li><a href=https://zhuanlan.zhihu.com/p/84734784 title=文本摘要简述>文本摘要简述</a>
生成式摘要面临的一些常见问题：但是简单的Seq2seq直接应用到摘要生成会有一些问题，比如重复生成、信息冗余，无法处理未登录词，关键信息丢失，可读性差等等。相对应的改进主要有以下几类：</li><li>从16年~19年，摘要任务持续走热，提出的新模型也是不胜枚举。左右摘要任务性能的关键点到底在哪呢？ACL19这篇文章对此做了探讨。 <a href=http://pfliu.com/InterpretSum/interpretSum.html title="Searching for Effective Neural Extractive Summarization: What Works and What’s Next">Searching for Effective Neural Extractive Summarization: What Works and What’s Next</a></li></ul><hr><h3 id=一些基于深度学习的自动摘要的sota>一些基于深度学习的自动摘要的SOTA
<a class=anchor href=#%e4%b8%80%e4%ba%9b%e5%9f%ba%e4%ba%8e%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e8%87%aa%e5%8a%a8%e6%91%98%e8%a6%81%e7%9a%84sota>#</a></h3><h4 id=textsum>Textsum
<a class=anchor href=#textsum>#</a></h4><p>2016 年，谷歌也开源了基于 TensorFlow的一个自动摘要模块 Textsum</p><h4 id=unilm-微软2020>UniLM ，微软，2020
<a class=anchor href=#unilm-%e5%be%ae%e8%bd%af2020>#</a></h4><ul><li><a href=https://www.shuzix.com/17176.html title="微软AI模型UniLM 在摘要和语言生成上实现超越">微软AI模型UniLM 在摘要和语言生成上实现超越</a></li></ul><h4 id=bertsum-2019-年的sota>BertSum, 2019 年的SOTA
<a class=anchor href=#bertsum-2019-%e5%b9%b4%e7%9a%84sota>#</a></h4><ul><li><a href=https://github.com/nlpyang/BertSum title=代码实现>github code</a> , <a href=https://arxiv.org/pdf/1903.10318.pdf title=论文>arxiv paper</a>, <a href=https://zhuanlan.zhihu.com/p/112282988>https://zhuanlan.zhihu.com/p/112282988</a></li></ul><h4 id=matchsum复旦大学2020年的sota>matchsum，复旦大学，2020年的SOTA
<a class=anchor href=#matchsum%e5%a4%8d%e6%97%a6%e5%a4%a7%e5%ad%a62020%e5%b9%b4%e7%9a%84sota>#</a></h4><ul><li><a href=https://github.com/maszhongming/MatchSum title=code>github code</a>， <a href=https://arxiv.org/pdf/2004.08795.pdf title=arxiv>arxiv paper</a>， <a href=https://www.bilibili.com/video/av200402243/ title=知乎上作者自己的讲解视频>作者自己的视频讲解</a></li></ul><h4 id=pegasus2020年google-research-的sota给出了超越人类水平的摘要性能>PEGASUS，2020年google research 的SOTA，给出了超越人类水平的摘要性能！！！
<a class=anchor href=#pegasus2020%e5%b9%b4google-research-%e7%9a%84sota%e7%bb%99%e5%87%ba%e4%ba%86%e8%b6%85%e8%b6%8a%e4%ba%ba%e7%b1%bb%e6%b0%b4%e5%b9%b3%e7%9a%84%e6%91%98%e8%a6%81%e6%80%a7%e8%83%bd>#</a></h4><ul><li><p><a href=https://arxiv.org/abs/1912.08777 title=论文地址>论文地址</a>, <a href=http://www.weainfo.net/news/detail/461443 title=论文的中文翻译>论文的中文翻译</a>, <a href=https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html title=博客地址>官方博客地址</a></p></li><li><p><a href=https://github.com/google-research/pegasus title=实现代码>实现代码</a>, <a href=https://github.com/TheRockXu/pegasus-demo title="非官方tensorflow 2.2版本 ">非官方tensorflow 2.2版本 </a>，<a href=https://github.com/bondarchukb/PEGASUS title="非官方tensorflow 2.3版本">非官方tensorflow 2.3版本</a>， <a href=https://github.com/ZhuiyiTechnology/t5-pegasus title=中文版本的pegasus,>中文版本的pegasus,一次性生成缺失的1/4文本</a></p></li></ul><h5 id=总言之pegasus-谷歌工具中穷人版的好东西对它有挺高期望也花了不少时间研究但由于它苛刻的生成形式并不觉得可以作为摘要生成来使用>总言之，PEGASUS 谷歌工具中穷人版的好东西。对它有挺高期望，也花了不少时间研究。但由于它苛刻的生成形式，并不觉得可以作为摘要生成来使用
<a class=anchor href=#%e6%80%bb%e8%a8%80%e4%b9%8bpegasus-%e8%b0%b7%e6%ad%8c%e5%b7%a5%e5%85%b7%e4%b8%ad%e7%a9%b7%e4%ba%ba%e7%89%88%e7%9a%84%e5%a5%bd%e4%b8%9c%e8%a5%bf%e5%af%b9%e5%ae%83%e6%9c%89%e6%8c%ba%e9%ab%98%e6%9c%9f%e6%9c%9b%e4%b9%9f%e8%8a%b1%e4%ba%86%e4%b8%8d%e5%b0%91%e6%97%b6%e9%97%b4%e7%a0%94%e7%a9%b6%e4%bd%86%e7%94%b1%e4%ba%8e%e5%ae%83%e8%8b%9b%e5%88%bb%e7%9a%84%e7%94%9f%e6%88%90%e5%bd%a2%e5%bc%8f%e5%b9%b6%e4%b8%8d%e8%a7%89%e5%be%97%e5%8f%af%e4%bb%a5%e4%bd%9c%e4%b8%ba%e6%91%98%e8%a6%81%e7%94%9f%e6%88%90%e6%9d%a5%e4%bd%bf%e7%94%a8>#</a></h5><h2 id=2021---04---01-文本摘要>2021 - 04 -01 文本摘要
<a class=anchor href=#2021---04---01-%e6%96%87%e6%9c%ac%e6%91%98%e8%a6%81>#</a></h2><p>2021-08-21 22:27:32 星期六
超级高效，好用的文本摘要算法
<a href=https://blog.csdn.net/malefactor/article/details/51264244>https://blog.csdn.net/malefactor/article/details/51264244</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#传统算法>传统算法</a></li><li><a href=#实用的自动摘要工具包>实用的自动摘要工具包</a></li><li><a href=#基于神经网络和深度学习的摘要方案>基于神经网络和深度学习的摘要方案</a></li><li><a href=#一些基于深度学习的自动摘要的sota>一些基于深度学习的自动摘要的SOTA</a></li></ul></li><li><a href=#2021---04---01-文本摘要>2021 - 04 -01 文本摘要</a></li></ul></li></ul></nav></div></aside></main></body></html>